{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_CNN_Introduction.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/limited-fion/deep-learning-cw/blob/master/02_CNN_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uoxdNjkDw2K"
      },
      "source": [
        "# Introduction to Convolutional Neural Networks\n",
        "\n",
        "This tutorial addresses the basic concepts regarding Convolutional Neural Networks and their implementation using the Keras framework.\n",
        "Convolutional Neural Networks (CNNs) are a class of feed-forward artificial neural architecture. They are applied to analyse visual 2D imagery, meaning that we can feed images directly into a CNN without the need to flatten them into a 1D vector as done in the previous tutorial.\n",
        "CNNs have revolutionised the field of computer vision in the last decade. In 2012 Alex Krizhevsky introduced the AlexNet architecture to win the ImageNet Challenge (one of the most important competitions on image classification within the Computer Vision community), by reducing the top-5 error more than 10 percentage points, which was an incredible improvement at that time. As of now, CNNs are used not only on image classification but in many other computer vision tasks.\n",
        "\n",
        "![](https://cdn-5f733ed3c1ac190fbc56ef88.closte.com/wp-content/uploads/2017/03/alexnet_small-1.png)\n",
        "\n",
        "The image above is from [cv-tricks' blog](https://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/) and shows the proposed AlexNet architecture. It is composed of 5 convolutional layers followed by 3 fully connected layers. Nowadays, we can find much deeper and more complex architectures, which outperform AlexNet on the ImageNet Challenge. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ2d_oQ2UEPz"
      },
      "source": [
        "## CNN Structure\n",
        "\n",
        "The basic pipeline of common CNNs consists of an image as input and a stack of convolutional layers that extract a feature representation from the input image. The final shape of the image representation is conditioned on the type of problem/task that the architecture is facing. For instance, the output of the last layer in a classification problem is a probability vector. Each dimension of the probability vector represents how likely is that the input image belongs to a specific class. However, the architecture design is up to us, and therefore, we could code a network that outputs a single value for regression problems, or that generates a new image map for semantic segmentation. Now, let's dig in a bit into CNN and introduce some layers that are widely used. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvNKve9GZL4M"
      },
      "source": [
        "### 2D Convolutional Layer\n",
        "\n",
        "The most common layer in any CNN architecture is the 2D convolutional layer. Convolutional layers are specifically designed to extract features from images or even extract features from previously extracted features. As shown in the following illustration, 2D convolutions apply the same filter to the full image. And therefore, due to its nature, 2D convolutional filters exploit the local information presented in images, making them a powerful tool for image analysis. \n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/800/1*Fw-ehcNBR9byHtho-Rxbtw.gif)\n",
        "\n",
        "Image [source](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1).\n",
        "\n",
        "The latest deep learning frameworks have made possible the integration of convolutional layers easily on our architectures with only a single line of code. We will address here how 2D convolutions work since full understanding is needed to comprehend how any CNN operates. The following images and some explanations can be further explored on the original [Irhum Shafkat's blog](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1) or in the [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) book. Both are strongly recommended.\n",
        "\n",
        "To understand 2D convolutions, we need to define first what a kernel is. Kernels are simply matrices of numbers. The numbers on the kernels are the so-called weights, and the weights on the kernels change as we train the network. Hence, network training aims to optimise the network's parameters (update the kernel weights) such that the cost function is minimised. \n",
        "\n",
        "The 2D convolution operation takes the network's kernels and \"slides\" them over the input image (alike to a sliding window) as in the following image from [PyImageSearch](https://www.pyimagesearch.com/2015/03/23/sliding-windows-for-object-detection-with-python-and-opencv/) blog:\n",
        "\n",
        "![](https://pyimagesearch.com/wp-content/uploads/2014/10/sliding_window_example.gif)\n",
        "\n",
        "In each step, the network performs an element-wise multiplication with the elements that are currently on. The results of this elementwise multiplication are added to obtain the output value of the operation. CNNs repeat previous step for all the positions of the sliding window, composing at the end the feature map. This generated feature map can go through another 2D convolutional layer and create more powerful features. \n",
        "\n",
        "> \n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/800/1*Zx-ZMLKab7VOCQTxdZ1OAw.gif)\n",
        "\n",
        "> \n",
        "The previous image shows the 2D convolution operation. Thus, the new feature values are the weighted sum of all the elements in the sliding window after the elementwise multiplication between input and kernel.  The bigger the size of the kernel is, the more feature elements contribute to the final output value. In contrast to fully connected layers, where a new feature value is a weighted sum over **all** input values, as mentioned, 2D convolutions compute features based on local areas. In other words, instead of looking at every input component, they consider only features coming from close locations. \n",
        "\n",
        "In the above example, the input image on the left has a size of 5x5 and the dimension of the resulting feature map is 3x3, showing that the size of the output maps is not always equal to the input. Hence, the output size can be computed by doing:\n",
        "\n",
        "$O = W - K + 1$,\n",
        "\n",
        "where $O$ is the output height/length, $W$ is the input height/length and $K$ is the kernel size. The output size is not only conditioned on the input size but also on the kernel size. Check in the following code cell how the output feature map shape changes as you increase the kernel size. In Keras, we define the layer by using `Conv2D` from `keras.layers` (documentation [here](https://keras.io/layers/convolutional/#conv2d)).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ez90vQEksR9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2f62fb2-a21a-4f5b-cc06-e1521169f698"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "\n",
        "# Generate dummy data\n",
        "input_feature = np.random.random((1, 100, 100, 1))\n",
        "\n",
        "# input: 100x100 image with 1 channels -> (100, 100, 1) tensor.\n",
        "# this applies 1 convolution filter of size 3x3 each.\n",
        "model = Sequential()\n",
        "model.add(Conv2D(1, (3, 3), input_shape=(100, 100, 1)))\n",
        "\n",
        "output_feature = model.predict(input_feature)\n",
        "\n",
        "print('Input size: ({:}, {:})'.format(input_feature.shape[1], input_feature.shape[2]))\n",
        "print('Output size: ({:}, {:})'.format(output_feature.shape[1], output_feature.shape[2]))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 12s 12s/step\n",
            "Input size: (100, 100)\n",
            "Output size: (98, 98)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNOhth4FuNob"
      },
      "source": [
        "Moreover, the kernel size and the input size are not the only parameters affecting the output size. We are going to introduce two extra elements that change the size of the output map: the padding and the stride."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT6f_n5QiU8X"
      },
      "source": [
        "### Adding Padding to Input Features\n",
        "\n",
        "In some tasks, such as [image translation](https://arxiv.org/pdf/1611.07004.pdf), we need the output size to be equal to the input size. The solution to that is using padding, where extra edges are added to the input features so that the dimension is not reduced after the convolutional layer. Normally those pixels have $0$ value (termed zero-padding), but depending on the application other methods could be used, e.g., reflection or symmetric padding.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/800/1*1okwhewf5KCtIPaFib4XaA.gif)\n",
        "\n",
        "If padding is used, the new output size can be computed by doing:\n",
        "\n",
        "$O = W - K + 2P+ 1$, \n",
        "\n",
        "where $P$ is the padding value. $P$ must be set in concordance with the kernel size if dimensionality wants to be preserved. Padding can be added to the `Conv2d` layer in Keras by using the padding argument and selecting one of the two settings: `valid`, which is the default value and means no padding; and `same`, which adapts the padding value to have the same output size as input size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Z5G2O00Etd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbed4db7-39ad-4cdb-b67c-852324f88575"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "\n",
        "# Generate dummy data\n",
        "input_feature = np.random.random((1, 100, 100, 1))\n",
        "\n",
        "# input: 100x100 image with 1 channels -> (100, 100, 1) tensor.\n",
        "# this applies 1 convolution filter of size 3x3 each.\n",
        "# attribute padding='same' applies zero-padding to the input feature map\n",
        "model = Sequential()\n",
        "model.add(Conv2D(1, (3, 3), input_shape=(100, 100, 1), padding='same'))\n",
        "\n",
        "output_feature = model.predict(input_feature)\n",
        "\n",
        "print('Input size: ({:}, {:})'.format(input_feature.shape[1], input_feature.shape[2]))\n",
        "print('Output size: ({:}, {:})'.format(output_feature.shape[1], output_feature.shape[2]))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 221ms/step\n",
            "Input size: (100, 100)\n",
            "Output size: (100, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdB4jMeswXVA"
      },
      "source": [
        "### Using Stride in Convolutional Layers\n",
        "\n",
        "The stride operation allows the convolutional layers to skip some of the sliding windows explained above. Hence, instead of jumping one pixel apart, we can define the number of skipped elements before computing the weighting sum between the kernel's weights and input features. A stride of 1 means that features will be extracted from all windows a pixel apart, so basically, every single window is computed. A stride of 2 means that we are selecting windows 2 pixels apart, skipping every other window in the process. Strides reduce the number of computations and consequently the size of the output map. In practice, as we go deeper into the CNN, the spatial size of the feature map gets smaller while the number of channels increases. Moreover, we can further reduce the size of the feature map using pooling operations, which we introduce later in this tutorial.\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/800/1*BMngs93_rm2_BpJFH2mS0Q.gif)\n",
        "\n",
        "If strides are used, the new output size can be computed as:\n",
        "\n",
        "$O = \\dfrac{W - K + 2P}{S}+ 1$, \n",
        "\n",
        "where $S$ is the stride value. The stride is set in the layer by using the `strides` argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8GjztiH0hnw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3507d278-bc0b-4209-de7f-9cdd84dcfdeb"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "\n",
        "# Generate dummy data\n",
        "input_feature = np.random.random((1, 100, 100, 1))\n",
        "\n",
        "# input: 100x100 image with 1 channels -> (100, 100, 1) tensor.\n",
        "# this applies 1 convolution filter of size 3x3 each.\n",
        "# attribute padding='same' applies zero-padding to the input feature map\n",
        "# attribute strides=2 applies applies stride of 2\n",
        "model = Sequential()\n",
        "model.add(Conv2D(1, (3, 3), input_shape=(100, 100, 1), padding='same', strides=2))\n",
        "\n",
        "output_feature = model.predict(input_feature)\n",
        "\n",
        "print('Input size: ({:}, {:})'.format(input_feature.shape[1], input_feature.shape[2]))\n",
        "print('Output size: ({:}, {:})'.format(output_feature.shape[1], output_feature.shape[2]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 57ms/step\n",
            "Input size: (100, 100)\n",
            "Output size: (50, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-fKvgJ3H2gr"
      },
      "source": [
        "## Differences Between Kernel and Filters\n",
        "\n",
        "The examples above take as input a single-channel image and compute a feature map with also one channel. However, when dealing with RGB images or feature maps, the input is no longer a single-channel map but, instead, they can have multiple channels. In the case of an RGB image, for each 2D convolution, we will need to define 3 kernels to interact with each of the image's channel colours. This group of kernels is called a filter. Thus, a filter is a collection of kernels that produces a single output.\n",
        "\n",
        "As a regular practice when defining Deep Learning models, we increase the number of filters in each convolutional layer as we go deeper into the model. Due to the element-wise multiplication, the number of kernels on each filter must be the same that the number of channels in the input feature map. Keras already deals with the number of kernels inside each filter. Keras keeps track of the input size in each convolutional layer unlike other frameworks (Pytorch or TensorFlow). Hence, in Keras, we must only decide the number of filters (output channels) in each layer.\n",
        "\n",
        "The next figure shows how the convolution is performed when having three input channels. First, one filter uses its three independent kernels to convolve with the RGB channels of the input image:\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*8dx6nxpUh2JqvYWPadTwMQ.gif)\n",
        "\n",
        "Next, each of the processed feature maps is added together to obtain a single channel:\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1000/1*CYB2dyR3EhFs1xNLK8ewiA.gif)\n",
        "\n",
        "Finally, we add the bias term to obtain the feature map. There is a single bias for the full output channel map. This operation is repeated for all the filters inside the convolutional layer.\n",
        "\n",
        "Now, we show how to use a `Conv2D` layer that takes an input image with 3 channels and generates an output map with 32 channels.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqiEwpdROxga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "305dbc1b-ae48-428a-c541-38ad3014c987"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "\n",
        "# Generate dummy data\n",
        "input_feature = np.random.random((1, 100, 100, 3))\n",
        "\n",
        "# input: 100x100 image with 3 channels -> (100, 100, 3) tensor.\n",
        "# this applies 32 convolution filters of size 3x3 each.\n",
        "# attribute padding='same' applies zero-padding to the input feature map\n",
        "# attribute strides=2 applies applies stride of 2\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(100, 100, 3), padding='same', strides=2))\n",
        "\n",
        "output_feature = model.predict(input_feature)\n",
        "\n",
        "print('Input size: ({:}, {:}, {:})'.format(input_feature.shape[1], input_feature.shape[2], input_feature.shape[3]))\n",
        "print('Output size: ({:}, {:}, {:})'.format(output_feature.shape[1], output_feature.shape[2], output_feature.shape[3]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 75ms/step\n",
            "Input size: (100, 100, 3)\n",
            "Output size: (50, 50, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S50cYwkhPqka"
      },
      "source": [
        "## Activation Functions\n",
        "\n",
        "As seen in previous tutorials, after a `Dense` layer we usually can find an activation function. We introduce here how to use them after `Conv2D` layers. Those activation functions are a set of operators that maps the feature values to a new set of values, the mapping depends on the function at hand. The main reason for using activation functions is that they add non-linearities to the network, giving more expressive power to the network, which will be able to reproduce more complex functions. \n",
        "\n",
        "\n",
        "The complete list of activation functions that Keras offers can be found [here](https://keras.io/activations/). We introduce here some of them:\n",
        "\n",
        "*  **Sigmoid Function** sets the output in the range (0, 1). The sigmoid function is widely used in binary classification problems since its output can be taken as a probability value. `keras.activations.sigmoid(x)`:\n",
        "\n",
        ">![](https://i.ibb.co/Ph8dsTv/sigmoid.png)\n",
        "\n",
        "*  **Tanh Function** is a logistic function as sigmoid, but the range of the tanh function is (-1, 1). Contrary to sigmoid function, where the values close to 0 are set around 0.5, in the tanh function they will be still mapped around the 0 value. `keras.activations.tanh(x)`:\n",
        "\n",
        ">![](https://i.ibb.co/68g7LpL/tanh.png)\n",
        "\n",
        "*  **ReLU Function** is the most common activation function you can find in any current CNN as in general works better than the rest. The range of this function is in \\[0, inf). It sets all negative values to 0 and hence is computationally easy to implement. As a drawback, during training some neurons *die*, meaning that the output is 0 for all available data points and no gradient is propagated there. `keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0.0)`:\n",
        "\n",
        ">![](https://i.ibb.co/Zd9H8Z4/relu.png)\n",
        "\n",
        "*  **LeakyReLU Function** is a modified version of the ReLU activation above, which attempts to solve the problem of dying neurons that ReLU has. While ReLU does not backpropagate negative values, Leaky ReLU smooths those values without setting them to 0. That allows the gradients to backpropagate through the network even for negative values. `keras.layers.LeakyReLU(alpha=0.3)`:\n",
        "\n",
        ">![](https://i.ibb.co/dmnJ6h1/leakyrelu.png)\n",
        "\n",
        "*  **Softmax Function** is another widely activation function for multi-class classification problems and usually is employed as the last activation function in the classification model. This function sets all of the output elements to the range (0, 1). However, the softmax function does not take independently the input values to map it into its probability value. Softmax Function takes an un-normalized vector, $s$, and normalizes it into a probability distribution, $p$, following the softmax expression. As the output is a probability, the output elements add up to 1. `keras.activations.softmax(x, axis=-1)`. Thus, the output value $p_i$ is computed as:\n",
        "\n",
        "> $p_{i} = \\dfrac{e^{s_i}}{\\sum_{\\substack{j}}^{N} e^{s_j}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MWo2VsYr5T2"
      },
      "source": [
        "The following example shows the feature maps before and after of the ReLU activation function. All values that are negative are set to 0 after the activation function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp1HH0VOqN0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6debdb7a-68df-4450-d8df-c3d5b2d270a7"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, Activation\n",
        "\n",
        "# Generate dummy data\n",
        "input_feature = np.random.random((1, 3, 3, 1)) - 0.5\n",
        "\n",
        "# input: 3x3 image with 1 channel -> (3, 3, 1) tensor.\n",
        "# This applies a 1 convolution filter of size 3x3 each.\n",
        "# This applies a ReLU activation function\n",
        "model = Sequential()\n",
        "model.add(Conv2D(1, (3, 3), input_shape=(3, 3, 1), padding='same', strides=1, name='conv'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model_before_ReLU = Model(inputs=model.input, outputs=model.get_layer('conv').output)\n",
        "\n",
        "output_feature = model_before_ReLU.predict(input_feature)\n",
        "output_ReLu_feature = model.predict(input_feature)\n",
        "\n",
        "print('Output Network without activation function')\n",
        "print(output_feature)\n",
        "\n",
        "print('')\n",
        "print('Output Network after ReLU activation function')\n",
        "print(output_ReLu_feature)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 122ms/step\n",
            "1/1 [==============================] - 0s 64ms/step\n",
            "Output Network without activation function\n",
            "[[[[-0.1285749 ]\n",
            "   [ 0.13717073]\n",
            "   [-0.46456495]]\n",
            "\n",
            "  [[-0.3495824 ]\n",
            "   [-0.30257282]\n",
            "   [-0.23059608]]\n",
            "\n",
            "  [[ 0.29468232]\n",
            "   [ 0.24366759]\n",
            "   [-0.04371584]]]]\n",
            "\n",
            "Output Network after ReLU activation function\n",
            "[[[[0.        ]\n",
            "   [0.13717073]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.        ]\n",
            "   [0.        ]\n",
            "   [0.        ]]\n",
            "\n",
            "  [[0.29468232]\n",
            "   [0.24366759]\n",
            "   [0.        ]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys6ZB1DOTLH6"
      },
      "source": [
        "## Pooling Layer\n",
        "\n",
        "It is a common practice to insert a pooling layer between convolutional layers in CNNs. In a standard CNN architecture, we set the feature sizes to become smaller progressively to reduce the computation in the networks, and to merge the information from different spatial locations. To reduce the feature map sizes, we can either use bigger stride size in the convolutional layers or we can use pooling layers. Pooling layers perform a spatial sliding window and apply an operation to reduce the spatial size. Those operations vary depending on the architecture, being the max, mean and min pooling the most typical ones. Here, we will explain the max pooling, although all the others work similarly. Max pooling keeps only the max value in a neighbourhood, where the neighbourhood is defined by the size of the kernel. Let's visualise it, the next example shows the result of a Max Pooling layer with a 2x2 kernel and a stride of 2.\n",
        "\n",
        "![](https://i.ibb.co/Xp454S4/MaxPool.png)\n",
        "\n",
        "As in convolutional layers, the final size is conditioned to the stride size of the pooling layer. However, contrary to convolutional layers, pooling layers operate independently on each of the input channels, without modifying the depth of the feature maps. To add max pooling to our model we need to import `MaxPooling2d` from `keras.layers` and define the stride and pooling size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzoTGOwDmQm4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edbac47a-a773-4412-e2ba-dd8130fff305"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Activation, MaxPooling2D\n",
        "\n",
        "# Generate dummy data\n",
        "input_feature = np.random.random((1, 100, 100, 3))\n",
        "\n",
        "# input: 100x100 image with 3 channels -> (100, 100, 3) tensor.\n",
        "# this applies 32 convolution filters of size 3x3 each.\n",
        "# attribute padding='same' applies zero-padding to the input feature map\n",
        "# attribute strides=1 applies applies stride of 1\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(100, 100, 3), padding='same', strides=1))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
        "\n",
        "output_feature = model.predict(input_feature)\n",
        "\n",
        "print('Input size: ({:}, {:}, {:})'.format(input_feature.shape[1], input_feature.shape[2], input_feature.shape[3]))\n",
        "print('Output size: ({:}, {:}, {:})'.format(output_feature.shape[1], output_feature.shape[2], output_feature.shape[3]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 93ms/step\n",
            "Input size: (100, 100, 3)\n",
            "Output size: (50, 50, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKU4-GbZHw0R"
      },
      "source": [
        "# Example: Classification on MNIST\n",
        "\n",
        "As opposed to the *Introduction to Keras* tutorial, in this section, we show how to perform image classification when the input data is a 2D image instead of a flat 1D vector. \n",
        "\n",
        "As discussed above, Convolutional Neural Networks aim to extract and exploit the local relationships on 2D maps, hence, CNNs are much more convenient for images than Multi-layer Perceptron models. \n",
        "\n",
        "First of all, we load the MNIST dataset from Keras' framework. The definition of the data is almost identical than in the Keras tutorial, although this time we are not reshaping the input images into a 1D vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3t0Wq9ArL4EZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a9300e3-d7ed-4214-84de-5c60f5814351"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import mnist\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
        "import numpy as np\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "# Here we are using the official test set as our validation set, in further\n",
        "# tutorials, test and validation splits will be explained properly.\n",
        "# Hence, even though the variables are `x_test` and `y_test`, they represent our validation set\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "shape = x_train.shape\n",
        "\n",
        "# Normalize and reshape the input images\n",
        "x_train = np.expand_dims(x_train.astype('float32'), -1)\n",
        "x_test = np.expand_dims(x_test.astype('float32'), -1)\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n",
            "Image shape: (28, 28, 1)\n",
            "Total number of training samples: 60000\n",
            "Total number of validation samples: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wp29GszMO-j"
      },
      "source": [
        "As illustrated in the cell, this time the input image before the model is 28x28x1. \n",
        "\n",
        "Now we can define a model composed of convolutional layers, activation functions, and maxpool operators:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B78NUAqwMeFR"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(16, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(16, (3,3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX_KAFlBMnVZ"
      },
      "source": [
        "As explained in the previous tutorial, in a classification problem, the output of the model is a probability vector. Each dimension of the vector indicates how likely is that the input image belongs to a specific class. \n",
        "\n",
        "Up to now, the resulting feature map of the model is a map with the shape *Batch x Weight' x Height' x Channel*, and it needs to be mapped into a vector with shape *Batch x Num Classes*. A common technique to process this mapping is to add a Flatten layer that will reshape the feature map to *Batch x (Weight' * Height' * Channel)*. Following the Flatten layer, we add a dense layer which maps this new feature map to the desired output size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CAc1r2vOJQv"
      },
      "source": [
        "model.add(Flatten())\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHInEJ-aON5v"
      },
      "source": [
        "Let's visualize the output shape in each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD4NZtIxOTbY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcb9712a-349f-4789-f26a-4e75b50d8662"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_5 (Conv2D)           (None, 28, 28, 16)        160       \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 28, 28, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 14, 14, 16)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 14, 14, 16)        2320      \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 14, 14, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 7, 7, 16)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                7850      \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,330\n",
            "Trainable params: 10,330\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F66ds58OTBu"
      },
      "source": [
        "Finally, we can train our CNN and check its performance on MNIST. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmeolsG_Pi8A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "290f35aa-c7f2-4bdf-9dbf-d137c2d681ee"
      },
      "source": [
        "# initiate RMSprop optimizer\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=10,  verbose=0)\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.08020985871553421\n",
            "Validation accuracy: 0.9751999974250793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0ZorPBtpLlZ"
      },
      "source": [
        "# Coursework\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gkM3dPZbNcX"
      },
      "source": [
        "## Task 1: Classification\n",
        "\n",
        "At this point, we know what is a CNN, how they work, and the components needed to design them. In this first task, we want you to create a CNN that is able to outperform the Multi-layer Perceptron model from Tutorial 1. For the first part of the coursework, we train on CIFAR10, a  classical dataset for image classification. Note that in these tutorials, we mainly use the official test sets of several standard datasets as our validation data. The reason we use the given test sets as validation data for the tutorials is that is an easy way to make sure that we all work with the same split and report results using the same data. However, in a proper machine learning setup, your validation set should be separate from the test set, so you can tune the model/parameters on the validation set and then check the final performance in the test set. Thus, even though the variables are `x_test` and `y_test`, they represent our validation set.\n",
        "\n",
        "Let's first load the dataset and visualise some examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKWvv0VwZPH3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "fbec1c9f-423b-4f63-a055-f7bbbf91a648"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# the data, shuffled and split between train and test sets\n",
        "# Here we are using the official test set as our validation set, in further\n",
        "# tutorials, test and validation splits will be explained properly.\n",
        "# Hence, even though the variables are `x_test` and `y_test`, they represent our validation set\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(X_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(X_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(X_test.shape[0]))\n",
        "\n",
        "# Let's visualize some examples\n",
        "N=5\n",
        "start_val = 0 # pick an element for the code to plot the following N**2 values\n",
        "fig, axes = plt.subplots(N,N)\n",
        "items = list(range(0, 10))\n",
        "for row in range(N):\n",
        "  for col in range(N):\n",
        "    idx = start_val+row+N*col\n",
        "    axes[row,col].imshow(X_train[idx], cmap='gray')\n",
        "    fig.subplots_adjust(hspace=0.5)\n",
        "    y_target = int(y_train[idx])\n",
        "    target = str(items[y_target])\n",
        "    axes[row,col].set_title(target)\n",
        "    axes[row,col].set_xticks([])\n",
        "    axes[row,col].set_yticks([])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 13s 0us/step\n",
            "Image shape: (32, 32, 3)\n",
            "Total number of training samples: 50000\n",
            "Total number of validation samples: 10000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 25 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-p9uVP3pccOW"
      },
      "source": [
        "CIFAR10 contains 50,000 32x32 color training images, labeled over 10 categories and 10,000 test images. As mentioned, we use those test images as our validation set in this tutorial. We need to preprocess the images for the 1D neural network defined in the previous tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFurVCqpclnm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43ac96ba-6bd0-426f-d0e2-015c97063a95"
      },
      "source": [
        "# Preprocess dataset for a 1D neural network\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# Normalize the image\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "X_train_flatten = X_train.reshape(X_train.shape[0], X_train.shape[1] * X_train.shape[2]*3)\n",
        "X_test_flatten = X_test.reshape(X_test.shape[0], X_test.shape[1] * X_test.shape[2]*3)\n",
        "\n",
        "print('New X_train shape: {0}'.format(X_train_flatten.shape))\n",
        "\n",
        "Y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "Y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "print('New Y_train shape: {0}'.format(Y_train_class.shape))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New X_train shape: (50000, 3072)\n",
            "New Y_train shape: (50000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxFx2s3Fc6wM"
      },
      "source": [
        "Now, we are ready to define the Multi-layer Perceptron model and train it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEZW-1pxbKlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d00624-572b-4271-de98-2d7544279db3"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(32, activation='relu', input_shape=(X_train_flatten.shape[1:])))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train_flatten, Y_train_class, batch_size=32, epochs=20)\n",
        "\n",
        "score = model.evaluate(X_test_flatten, Y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.9454 - accuracy: 0.2879\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.7721 - accuracy: 0.3567\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6954 - accuracy: 0.3854\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6420 - accuracy: 0.4103\n",
            "Epoch 5/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6014 - accuracy: 0.4253\n",
            "Epoch 6/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5657 - accuracy: 0.4397\n",
            "Epoch 7/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5389 - accuracy: 0.4496\n",
            "Epoch 8/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5119 - accuracy: 0.4586\n",
            "Epoch 9/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4910 - accuracy: 0.4668\n",
            "Epoch 10/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4729 - accuracy: 0.4736\n",
            "Epoch 11/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4556 - accuracy: 0.4796\n",
            "Epoch 12/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4400 - accuracy: 0.4847\n",
            "Epoch 13/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4243 - accuracy: 0.4892\n",
            "Epoch 14/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4102 - accuracy: 0.4975\n",
            "Epoch 15/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3955 - accuracy: 0.5024\n",
            "Epoch 16/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3854 - accuracy: 0.5069\n",
            "Epoch 17/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3702 - accuracy: 0.5127\n",
            "Epoch 18/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3586 - accuracy: 0.5142\n",
            "Epoch 19/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3467 - accuracy: 0.5198\n",
            "Epoch 20/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3356 - accuracy: 0.5238\n",
            "Validation loss: 1.4425725936889648\n",
            "Validation accuracy: 0.4925999939441681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEuYNmyHlVJd"
      },
      "source": [
        "### Problem Definition\n",
        "\n",
        "In this exercise, you are asked to test several CNN architectures in the code provided below. Do not modify the optimizer, loss used or parameters related to the training such as the learning rate, they will be investigated in future tutorials. You must focus on the architecture itself: number of convolutional layers, number of filters in every layer, activation functions, pooling operators, among others. Batch Normalization and Dropout layers, which are quite used in CNN architectures, will be also investigated in a future tutorial so you do not have to discuss them.\n",
        "\n",
        "\n",
        "**Report**:\n",
        "*   Present a bar figure with the training and validation accuracies for different design choices. Discuss only the parameters that have a significant influence on the network's performance. Explain any discrepancy between training and validation accuracies.\n",
        "*   Present a sketch that introduces your best architecture. See some examples on how to display networks in [cv-tricks' blog](https://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 1 : Number of layers**\n",
        "1.   16\n",
        "2.   16, 32\n",
        "3.   16, 32, 64\n",
        "4.   64, 128\n",
        "5.   64, 128, 256\n",
        "6.   64\n",
        "7.   128, 256\n",
        "8.   128, 256, 512\n"
      ],
      "metadata": {
        "id": "73vPOEi2iTsP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8ATD38dDM1q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9094e66b-a68c-4e57-f135-e676587cf56e"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, AveragePooling2D\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# the data, shuffled and split between train and val sets\n",
        "# Here we are using the official test set as our validation set, in further\n",
        "# tutorials, test and validation splits will be explained properly.\n",
        "# Hence, even though the variables are `x_test` and `y_test`, they represent our validation set\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=20, validation_data=(x_test, y_test_class))\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "model.save('/MyDrive/dl-cw/layer-1') \n",
        "del model "
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Image shape: (32, 32, 3)\n",
            "Total number of training samples: 50000\n",
            "Total number of validation samples: 10000\n",
            "Model: \"sequential_46\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_102 (Conv2D)         (None, 32, 32, 16)        448       \n",
            "                                                                 \n",
            " activation_102 (Activation)  (None, 32, 32, 16)       0         \n",
            "                                                                 \n",
            " max_pooling2d_100 (MaxPooli  (None, 16, 16, 16)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " average_pooling2d_42 (Avera  (None, 8, 8, 16)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_46 (Flatten)        (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_92 (Dense)            (None, 50)                51250     \n",
            "                                                                 \n",
            " dense_93 (Dense)            (None, 10)                510       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 52,208\n",
            "Trainable params: 52,208\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.0311 - accuracy: 0.2680 - val_loss: 1.8931 - val_accuracy: 0.3312\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.8216 - accuracy: 0.3602 - val_loss: 1.7740 - val_accuracy: 0.3829\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.7343 - accuracy: 0.3977 - val_loss: 1.6993 - val_accuracy: 0.4051\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.6699 - accuracy: 0.4172 - val_loss: 1.6295 - val_accuracy: 0.4313\n",
            "Epoch 5/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.6144 - accuracy: 0.4354 - val_loss: 1.6110 - val_accuracy: 0.4272\n",
            "Epoch 6/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5686 - accuracy: 0.4478 - val_loss: 1.5479 - val_accuracy: 0.4588\n",
            "Epoch 7/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5272 - accuracy: 0.4638 - val_loss: 1.5129 - val_accuracy: 0.4594\n",
            "Epoch 8/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4900 - accuracy: 0.4745 - val_loss: 1.4681 - val_accuracy: 0.4839\n",
            "Epoch 9/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4561 - accuracy: 0.4885 - val_loss: 1.4386 - val_accuracy: 0.4908\n",
            "Epoch 10/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4263 - accuracy: 0.4979 - val_loss: 1.4236 - val_accuracy: 0.4963\n",
            "Epoch 11/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4015 - accuracy: 0.5067 - val_loss: 1.4092 - val_accuracy: 0.4964\n",
            "Epoch 12/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3786 - accuracy: 0.5157 - val_loss: 1.3879 - val_accuracy: 0.5027\n",
            "Epoch 13/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3584 - accuracy: 0.5242 - val_loss: 1.3596 - val_accuracy: 0.5158\n",
            "Epoch 14/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3409 - accuracy: 0.5303 - val_loss: 1.3543 - val_accuracy: 0.5135\n",
            "Epoch 15/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3250 - accuracy: 0.5358 - val_loss: 1.3406 - val_accuracy: 0.5201\n",
            "Epoch 16/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3107 - accuracy: 0.5392 - val_loss: 1.3082 - val_accuracy: 0.5349\n",
            "Epoch 17/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2963 - accuracy: 0.5467 - val_loss: 1.3053 - val_accuracy: 0.5399\n",
            "Epoch 18/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2834 - accuracy: 0.5502 - val_loss: 1.2895 - val_accuracy: 0.5455\n",
            "Epoch 19/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2726 - accuracy: 0.5541 - val_loss: 1.2774 - val_accuracy: 0.5494\n",
            "Epoch 20/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2604 - accuracy: 0.5591 - val_loss: 1.2781 - val_accuracy: 0.5488\n",
            "Validation loss: 1.2781280279159546\n",
            "Validation accuracy: 0.548799991607666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(32, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=20, validation_data=(x_test, y_test_class))\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "model.save('/MyDrive/dl-cw/layer-2') \n",
        "del model "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8dE2FGLhwZv",
        "outputId": "0f2a1097-6b4c-4e38-dda9-8600ba93aee0"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: (32, 32, 3)\n",
            "Total number of training samples: 50000\n",
            "Total number of validation samples: 10000\n",
            "Model: \"sequential_47\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_103 (Conv2D)         (None, 32, 32, 16)        448       \n",
            "                                                                 \n",
            " activation_103 (Activation)  (None, 32, 32, 16)       0         \n",
            "                                                                 \n",
            " max_pooling2d_101 (MaxPooli  (None, 16, 16, 16)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_104 (Conv2D)         (None, 16, 16, 32)        4640      \n",
            "                                                                 \n",
            " activation_104 (Activation)  (None, 16, 16, 32)       0         \n",
            "                                                                 \n",
            " max_pooling2d_102 (MaxPooli  (None, 8, 8, 32)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " average_pooling2d_43 (Avera  (None, 4, 4, 32)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_47 (Flatten)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_94 (Dense)            (None, 50)                25650     \n",
            "                                                                 \n",
            " dense_95 (Dense)            (None, 10)                510       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,248\n",
            "Trainable params: 31,248\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.0096 - accuracy: 0.2839 - val_loss: 1.8518 - val_accuracy: 0.3602\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.7893 - accuracy: 0.3689 - val_loss: 1.7316 - val_accuracy: 0.3955\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6835 - accuracy: 0.4040 - val_loss: 1.6336 - val_accuracy: 0.4273\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6062 - accuracy: 0.4304 - val_loss: 1.5752 - val_accuracy: 0.4456\n",
            "Epoch 5/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5473 - accuracy: 0.4518 - val_loss: 1.5176 - val_accuracy: 0.4596\n",
            "Epoch 6/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4989 - accuracy: 0.4710 - val_loss: 1.4632 - val_accuracy: 0.4832\n",
            "Epoch 7/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4576 - accuracy: 0.4847 - val_loss: 1.4299 - val_accuracy: 0.4977\n",
            "Epoch 8/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4203 - accuracy: 0.4958 - val_loss: 1.3967 - val_accuracy: 0.4990\n",
            "Epoch 9/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3885 - accuracy: 0.5077 - val_loss: 1.3900 - val_accuracy: 0.5037\n",
            "Epoch 10/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3599 - accuracy: 0.5188 - val_loss: 1.3503 - val_accuracy: 0.5207\n",
            "Epoch 11/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3351 - accuracy: 0.5286 - val_loss: 1.3195 - val_accuracy: 0.5353\n",
            "Epoch 12/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3133 - accuracy: 0.5353 - val_loss: 1.3064 - val_accuracy: 0.5356\n",
            "Epoch 13/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2938 - accuracy: 0.5428 - val_loss: 1.3121 - val_accuracy: 0.5271\n",
            "Epoch 14/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2767 - accuracy: 0.5512 - val_loss: 1.2814 - val_accuracy: 0.5428\n",
            "Epoch 15/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2613 - accuracy: 0.5571 - val_loss: 1.2692 - val_accuracy: 0.5493\n",
            "Epoch 16/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2464 - accuracy: 0.5602 - val_loss: 1.2335 - val_accuracy: 0.5613\n",
            "Epoch 17/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2337 - accuracy: 0.5657 - val_loss: 1.2265 - val_accuracy: 0.5604\n",
            "Epoch 18/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2209 - accuracy: 0.5717 - val_loss: 1.2148 - val_accuracy: 0.5663\n",
            "Epoch 19/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2090 - accuracy: 0.5757 - val_loss: 1.2117 - val_accuracy: 0.5704\n",
            "Epoch 20/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1972 - accuracy: 0.5798 - val_loss: 1.2897 - val_accuracy: 0.5416\n",
            "Validation loss: 1.2896801233291626\n",
            "Validation accuracy: 0.5415999889373779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, AveragePooling2D\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(16, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(32, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(64, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=1, validation_data=(x_test, y_test_class))\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "model.save('layer_3', save_format='h5') \n",
        "del model "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUb9y6cdi11u",
        "outputId": "ac5b5ac3-f975-4f32-93dc-a28772b5b806"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Image shape: (32, 32, 3)\n",
            "Total number of training samples: 50000\n",
            "Total number of validation samples: 10000\n",
            "Model: \"sequential_53\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_120 (Conv2D)         (None, 32, 32, 16)        448       \n",
            "                                                                 \n",
            " activation_120 (Activation)  (None, 32, 32, 16)       0         \n",
            "                                                                 \n",
            " max_pooling2d_118 (MaxPooli  (None, 16, 16, 16)       0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_121 (Conv2D)         (None, 16, 16, 32)        4640      \n",
            "                                                                 \n",
            " activation_121 (Activation)  (None, 16, 16, 32)       0         \n",
            "                                                                 \n",
            " max_pooling2d_119 (MaxPooli  (None, 8, 8, 32)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " conv2d_122 (Conv2D)         (None, 8, 8, 64)          18496     \n",
            "                                                                 \n",
            " activation_122 (Activation)  (None, 8, 8, 64)         0         \n",
            "                                                                 \n",
            " max_pooling2d_120 (MaxPooli  (None, 4, 4, 64)         0         \n",
            " ng2D)                                                           \n",
            "                                                                 \n",
            " average_pooling2d_49 (Avera  (None, 2, 2, 64)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_53 (Flatten)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_106 (Dense)           (None, 50)                12850     \n",
            "                                                                 \n",
            " dense_107 (Dense)           (None, 10)                510       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 36,944\n",
            "Trainable params: 36,944\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 2.0349 - accuracy: 0.2713 - val_loss: 1.8606 - val_accuracy: 0.3403\n",
            "Validation loss: 1.8605916500091553\n",
            "Validation accuracy: 0.3402999937534332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, AveragePooling2D\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(128, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=20, validation_data=(x_test, y_test_class))\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "model.save('/MyDrive/dl-cw/layer-3') \n",
        "del model "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THsFXL1mkGV-",
        "outputId": "27378858-68ae-4b98-d7b7-73820e9310aa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Image shape: (32, 32, 3)\n",
            "Total number of training samples: 50000\n",
            "Total number of validation samples: 10000\n",
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 1.7724 - accuracy: 0.3665\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4813 - accuracy: 0.4721\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3555 - accuracy: 0.5216\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2668 - accuracy: 0.5526\n",
            "Epoch 5/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.1923 - accuracy: 0.5807\n",
            "Epoch 6/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.1314 - accuracy: 0.6031\n",
            "Epoch 7/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0812 - accuracy: 0.6211\n",
            "Epoch 8/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0389 - accuracy: 0.6359\n",
            "Epoch 9/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0011 - accuracy: 0.6504\n",
            "Epoch 10/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9693 - accuracy: 0.6637\n",
            "Epoch 11/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9380 - accuracy: 0.6724\n",
            "Epoch 12/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9109 - accuracy: 0.6828\n",
            "Epoch 13/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8884 - accuracy: 0.6911\n",
            "Epoch 14/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8634 - accuracy: 0.7012\n",
            "Epoch 15/20\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8413 - accuracy: 0.7085\n",
            "Epoch 16/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8213 - accuracy: 0.7154\n",
            "Epoch 17/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8016 - accuracy: 0.7209\n",
            "Epoch 18/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7818 - accuracy: 0.7297\n",
            "Epoch 19/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7639 - accuracy: 0.7362\n",
            "Epoch 20/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7467 - accuracy: 0.7424\n",
            "Validation loss: 0.8741204142570496\n",
            "Validation accuracy: 0.7001000046730042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load('/MyDrive/dl-cw/layer-3') "
      ],
      "metadata": {
        "id": "CIuKkdJqiwFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, AveragePooling2D\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Test convolution-subsampling pairs \n",
        "# 16\n",
        "# 16, 32\n",
        "# 16, 32, 64\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(128, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(256, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=20)\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "model.save_weights('/content/drive/MyDrive/pairs_5_weights.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm8UHbrWkCEP",
        "outputId": "f19ec74c-90ad-4bad-dddd-3659a4ef2a34"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Image shape: (32, 32, 3)\n",
            "Total number of training samples: 50000\n",
            "Total number of validation samples: 10000\n",
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 1.7852 - accuracy: 0.3524\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4904 - accuracy: 0.4571\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3460 - accuracy: 0.5148\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2281 - accuracy: 0.5636\n",
            "Epoch 5/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1323 - accuracy: 0.6011\n",
            "Epoch 6/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0546 - accuracy: 0.6282\n",
            "Epoch 7/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.9952 - accuracy: 0.6507\n",
            "Epoch 8/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.9441 - accuracy: 0.6701\n",
            "Epoch 9/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8992 - accuracy: 0.6872\n",
            "Epoch 10/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8573 - accuracy: 0.7012\n",
            "Epoch 11/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8213 - accuracy: 0.7145\n",
            "Epoch 12/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7874 - accuracy: 0.7276\n",
            "Epoch 13/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7534 - accuracy: 0.7399\n",
            "Epoch 14/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7242 - accuracy: 0.7499\n",
            "Epoch 15/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6950 - accuracy: 0.7613\n",
            "Epoch 16/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6682 - accuracy: 0.7674\n",
            "Epoch 17/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6438 - accuracy: 0.7774\n",
            "Epoch 18/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6200 - accuracy: 0.7863\n",
            "Epoch 19/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.5963 - accuracy: 0.7943\n",
            "Epoch 20/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.5717 - accuracy: 0.8032\n",
            "Validation loss: 0.7432730197906494\n",
            "Validation accuracy: 0.7480000257492065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, AveragePooling2D\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=20)\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "model.save_weights('/content/drive/MyDrive/pairs_6_weights.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1TW-uTUnJf9",
        "outputId": "62960568-a08a-4f81-bb05-16745aeb2906"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Image shape: (32, 32, 3)\n",
            "Total number of training samples: 50000\n",
            "Total number of validation samples: 10000\n",
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.7709 - accuracy: 0.3800\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4905 - accuracy: 0.4794\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3636 - accuracy: 0.5265\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2845 - accuracy: 0.5538\n",
            "Epoch 5/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2273 - accuracy: 0.5757\n",
            "Epoch 6/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1805 - accuracy: 0.5927\n",
            "Epoch 7/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1443 - accuracy: 0.6020\n",
            "Epoch 8/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1109 - accuracy: 0.6158\n",
            "Epoch 9/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0812 - accuracy: 0.6242\n",
            "Epoch 10/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0545 - accuracy: 0.6343\n",
            "Epoch 11/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0308 - accuracy: 0.6417\n",
            "Epoch 12/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0089 - accuracy: 0.6526\n",
            "Epoch 13/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.9880 - accuracy: 0.6578\n",
            "Epoch 14/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.9678 - accuracy: 0.6653\n",
            "Epoch 15/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.9501 - accuracy: 0.6722\n",
            "Epoch 16/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.9322 - accuracy: 0.6813\n",
            "Epoch 17/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.9159 - accuracy: 0.6844\n",
            "Epoch 18/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.9013 - accuracy: 0.6908\n",
            "Epoch 19/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8866 - accuracy: 0.6941\n",
            "Epoch 20/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8723 - accuracy: 0.6992\n",
            "Validation loss: 0.9555184841156006\n",
            "Validation accuracy: 0.66839998960495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, AveragePooling2D\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(256, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=20)\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "model.save_weights('/content/drive/MyDrive/pairs_7_weights.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8tEkH-5nNJV",
        "outputId": "d34da8ee-a347-4348-d19a-bdc3fbcfb2c3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Image shape: (32, 32, 3)\n",
            "Total number of training samples: 50000\n",
            "Total number of validation samples: 10000\n",
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6854 - accuracy: 0.3963\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3682 - accuracy: 0.5133\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.2280 - accuracy: 0.5672\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1285 - accuracy: 0.6045\n",
            "Epoch 5/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0507 - accuracy: 0.6322\n",
            "Epoch 6/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9889 - accuracy: 0.6546\n",
            "Epoch 7/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9387 - accuracy: 0.6754\n",
            "Epoch 8/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8966 - accuracy: 0.6888\n",
            "Epoch 9/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8582 - accuracy: 0.7040\n",
            "Epoch 10/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8220 - accuracy: 0.7157\n",
            "Epoch 11/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7914 - accuracy: 0.7277\n",
            "Epoch 12/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7640 - accuracy: 0.7360\n",
            "Epoch 13/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7358 - accuracy: 0.7465\n",
            "Epoch 14/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7112 - accuracy: 0.7541\n",
            "Epoch 15/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6849 - accuracy: 0.7625\n",
            "Epoch 16/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6602 - accuracy: 0.7717\n",
            "Epoch 17/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6382 - accuracy: 0.7809\n",
            "Epoch 18/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6160 - accuracy: 0.7879\n",
            "Epoch 19/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5946 - accuracy: 0.7962\n",
            "Epoch 20/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5739 - accuracy: 0.8045\n",
            "Validation loss: 0.7719926238059998\n",
            "Validation accuracy: 0.7378000020980835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, AveragePooling2D\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(256, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(512, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=20)\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "model.save_weights('/content/drive/MyDrive/dl-cw/pairs_8_weights.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z0EcylXoneP",
        "outputId": "81c26fe0-4e3e-4881-87fa-bd980d942770"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Image shape: (32, 32, 3)\n",
            "Total number of training samples: 50000\n",
            "Total number of validation samples: 10000\n",
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6795 - accuracy: 0.3877\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3193 - accuracy: 0.5272\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1324 - accuracy: 0.6005\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0029 - accuracy: 0.6497\n",
            "Epoch 5/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9140 - accuracy: 0.6821\n",
            "Epoch 6/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8406 - accuracy: 0.7082\n",
            "Epoch 7/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7757 - accuracy: 0.7339\n",
            "Epoch 8/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7203 - accuracy: 0.7516\n",
            "Epoch 9/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6680 - accuracy: 0.7716\n",
            "Epoch 10/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6218 - accuracy: 0.7849\n",
            "Epoch 11/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5778 - accuracy: 0.8035\n",
            "Epoch 12/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5404 - accuracy: 0.8142\n",
            "Epoch 13/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5010 - accuracy: 0.8284\n",
            "Epoch 14/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4643 - accuracy: 0.8430\n",
            "Epoch 15/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4288 - accuracy: 0.8544\n",
            "Epoch 16/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3962 - accuracy: 0.8649\n",
            "Epoch 17/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3621 - accuracy: 0.8784\n",
            "Epoch 18/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3283 - accuracy: 0.8886\n",
            "Epoch 19/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2981 - accuracy: 0.9002\n",
            "Epoch 20/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2696 - accuracy: 0.9100\n",
            "Validation loss: 0.7281681895256042\n",
            "Validation accuracy: 0.782800018787384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 2: How many feature maps?**\n",
        "1.   16, 32, 64\n",
        "2.   32, 64, 128\n",
        "3.   64, 128, 256\n",
        "4.   128, 256, 512\n",
        "5.   256, 512, 1024\n",
        "\n"
      ],
      "metadata": {
        "id": "jwmftedjjSM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalize the image\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(64, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(128, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=20)\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "model.save_weights('/content/drive/MyDrive/filters_2_weights.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPfOdcH1Nfyz",
        "outputId": "fac3ff02-4775-4dad-c25f-32845ee38757"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: (32, 32, 3)\n",
            "Total number of training samples: 50000\n",
            "Total number of validation samples: 10000\n",
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8811 - accuracy: 0.3254\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.6105 - accuracy: 0.4146\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4829 - accuracy: 0.4624\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3953 - accuracy: 0.4968\n",
            "Epoch 5/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3242 - accuracy: 0.5260\n",
            "Epoch 6/20\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2611 - accuracy: 0.5494\n",
            "Epoch 7/20\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.2051 - accuracy: 0.5709\n",
            "Epoch 8/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1517 - accuracy: 0.5919\n",
            "Epoch 9/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.1108 - accuracy: 0.6078\n",
            "Epoch 10/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0730 - accuracy: 0.6227\n",
            "Epoch 11/20\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 1.0387 - accuracy: 0.6349\n",
            "Epoch 12/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0085 - accuracy: 0.6463\n",
            "Epoch 13/20\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 0.9820 - accuracy: 0.6566\n",
            "Epoch 14/20\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.9572 - accuracy: 0.6642\n",
            "Epoch 15/20\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.9332 - accuracy: 0.6717\n",
            "Epoch 16/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9140 - accuracy: 0.6799\n",
            "Epoch 17/20\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8918 - accuracy: 0.6874\n",
            "Epoch 18/20\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8738 - accuracy: 0.6959\n",
            "Epoch 19/20\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8538 - accuracy: 0.7025\n",
            "Epoch 20/20\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8359 - accuracy: 0.7102\n",
            "Validation loss: 0.9116654396057129\n",
            "Validation accuracy: 0.6800000071525574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, AveragePooling2D\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalize the image\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(256, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(512, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(1024, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=20)\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "model.save_weights('/content/drive/MyDrive/filters_5_weights.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHuul9mXVPh5",
        "outputId": "dc11821a-9326-4f9c-c998-89840a0ac27c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: (32, 32, 3)\n",
            "Total number of training samples: 50000\n",
            "Total number of validation samples: 10000\n",
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 36s 22ms/step - loss: 1.5840 - accuracy: 0.4212\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 37s 24ms/step - loss: 1.1581 - accuracy: 0.5897\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.9612 - accuracy: 0.6642\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.8330 - accuracy: 0.7106\n",
            "Epoch 5/20\n",
            "1563/1563 [==============================] - 34s 21ms/step - loss: 0.7344 - accuracy: 0.7461\n",
            "Epoch 6/20\n",
            "1563/1563 [==============================] - 35s 23ms/step - loss: 0.6549 - accuracy: 0.7761\n",
            "Epoch 7/20\n",
            "1563/1563 [==============================] - 37s 24ms/step - loss: 0.5826 - accuracy: 0.8008\n",
            "Epoch 8/20\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.5192 - accuracy: 0.8233\n",
            "Epoch 9/20\n",
            "1563/1563 [==============================] - 37s 24ms/step - loss: 0.4609 - accuracy: 0.8427\n",
            "Epoch 10/20\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.4024 - accuracy: 0.8621\n",
            "Epoch 11/20\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.3484 - accuracy: 0.8808\n",
            "Epoch 12/20\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 0.2967 - accuracy: 0.8985\n",
            "Epoch 13/20\n",
            "1563/1563 [==============================] - 36s 23ms/step - loss: 0.2469 - accuracy: 0.9180\n",
            "Epoch 14/20\n",
            "1563/1563 [==============================] - 35s 22ms/step - loss: 0.2048 - accuracy: 0.9324\n",
            "Epoch 15/20\n",
            "1563/1563 [==============================] - 34s 21ms/step - loss: 0.1643 - accuracy: 0.9467\n",
            "Epoch 16/20\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.1306 - accuracy: 0.9569\n",
            "Epoch 17/20\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.1030 - accuracy: 0.9664\n",
            "Epoch 18/20\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0826 - accuracy: 0.9731\n",
            "Epoch 19/20\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0670 - accuracy: 0.9779\n",
            "Epoch 20/20\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 0.0553 - accuracy: 0.9822\n",
            "Validation loss: 1.1242856979370117\n",
            "Validation accuracy: 0.7802000045776367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment 3: How large a dense layer?\n",
        "1.   128\n",
        "2.   256\n",
        "3.   512\n",
        "4.   512, 256\n",
        "\n"
      ],
      "metadata": {
        "id": "kkDvKKrashZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalize the image\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(256, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(512, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=20)\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "model.save_weights('/content/drive/MyDrive/dense_1_weights.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8FKg0pnthM9",
        "outputId": "430a75bb-b681-4b53-9d2d-08896744d686"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: (32, 32, 3)\n",
            "Total number of training samples: 50000\n",
            "Total number of validation samples: 10000\n",
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 17s 10ms/step - loss: 1.6826 - accuracy: 0.3871\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 1.3488 - accuracy: 0.5153\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.1675 - accuracy: 0.5867\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.0429 - accuracy: 0.6333\n",
            "Epoch 5/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9524 - accuracy: 0.6663\n",
            "Epoch 6/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.8805 - accuracy: 0.6929\n",
            "Epoch 7/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.8194 - accuracy: 0.7160\n",
            "Epoch 8/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.7655 - accuracy: 0.7362\n",
            "Epoch 9/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.7176 - accuracy: 0.7540\n",
            "Epoch 10/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.6728 - accuracy: 0.7676\n",
            "Epoch 11/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.6352 - accuracy: 0.7812\n",
            "Epoch 12/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.5934 - accuracy: 0.7967\n",
            "Epoch 13/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.5553 - accuracy: 0.8103\n",
            "Epoch 14/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.5218 - accuracy: 0.8228\n",
            "Epoch 15/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.4898 - accuracy: 0.8323\n",
            "Epoch 16/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.4569 - accuracy: 0.8419\n",
            "Epoch 17/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.4258 - accuracy: 0.8541\n",
            "Epoch 18/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.3972 - accuracy: 0.8643\n",
            "Epoch 19/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.3658 - accuracy: 0.8783\n",
            "Epoch 20/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.3370 - accuracy: 0.8864\n",
            "Validation loss: 0.7627995014190674\n",
            "Validation accuracy: 0.7671999931335449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, AveragePooling2D\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalize the image\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(256, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(512, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=20)\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "model.save_weights('/content/drive/MyDrive/dense_3_weights.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9LSa0jDZJt5",
        "outputId": "65dc94fc-a8ea-4271-f38a-1363b595a580"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: (32, 32, 3)\n",
            "Total number of training samples: 50000\n",
            "Total number of validation samples: 10000\n",
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 1.6683 - accuracy: 0.3926\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 15s 10ms/step - loss: 1.2942 - accuracy: 0.5355\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.0972 - accuracy: 0.6130\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9706 - accuracy: 0.6622\n",
            "Epoch 5/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.8804 - accuracy: 0.6932\n",
            "Epoch 6/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.8026 - accuracy: 0.7239\n",
            "Epoch 7/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.7388 - accuracy: 0.7434\n",
            "Epoch 8/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.6799 - accuracy: 0.7651\n",
            "Epoch 9/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.6302 - accuracy: 0.7830\n",
            "Epoch 10/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.5807 - accuracy: 0.8012\n",
            "Epoch 11/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.5348 - accuracy: 0.8163\n",
            "Epoch 12/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.4919 - accuracy: 0.8308\n",
            "Epoch 13/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.4526 - accuracy: 0.8459\n",
            "Epoch 14/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.4128 - accuracy: 0.8594\n",
            "Epoch 15/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.3757 - accuracy: 0.8707\n",
            "Epoch 16/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.3389 - accuracy: 0.8842\n",
            "Epoch 17/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.3043 - accuracy: 0.8974\n",
            "Epoch 18/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.2716 - accuracy: 0.9093\n",
            "Epoch 19/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.2393 - accuracy: 0.9202\n",
            "Epoch 20/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.2076 - accuracy: 0.9308\n",
            "Validation loss: 0.77132648229599\n",
            "Validation accuracy: 0.7833999991416931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, AveragePooling2D\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalize the image\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(256, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(512, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=20)\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "model.save_weights('/content/drive/MyDrive/dense_4_weights.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "al3_97fgyJKU",
        "outputId": "d69c8b9e-cc3a-40c9-af15-2acee3be552e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: (32, 32, 3)\n",
            "Total number of training samples: 50000\n",
            "Total number of validation samples: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 16s 9ms/step - loss: 1.6857 - accuracy: 0.3825\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 1.3037 - accuracy: 0.5292\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 1.0875 - accuracy: 0.6113\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.9486 - accuracy: 0.6643\n",
            "Epoch 5/20\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.8439 - accuracy: 0.7031\n",
            "Epoch 6/20\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.7597 - accuracy: 0.7362\n",
            "Epoch 7/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.6840 - accuracy: 0.7612\n",
            "Epoch 8/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.6197 - accuracy: 0.7830\n",
            "Epoch 9/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.5630 - accuracy: 0.8056\n",
            "Epoch 10/20\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.5065 - accuracy: 0.8244\n",
            "Epoch 11/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.4517 - accuracy: 0.8434\n",
            "Epoch 12/20\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.4027 - accuracy: 0.8613\n",
            "Epoch 13/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.3547 - accuracy: 0.8775\n",
            "Epoch 14/20\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.3086 - accuracy: 0.8944\n",
            "Epoch 15/20\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.2648 - accuracy: 0.9085\n",
            "Epoch 16/20\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.2233 - accuracy: 0.9233\n",
            "Epoch 17/20\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.1889 - accuracy: 0.9354\n",
            "Epoch 18/20\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.1563 - accuracy: 0.9465\n",
            "Epoch 19/20\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.1275 - accuracy: 0.9571\n",
            "Epoch 20/20\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 0.1058 - accuracy: 0.9639\n",
            "Validation loss: 0.9248126149177551\n",
            "Validation accuracy: 0.7853000164031982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 4: Pooling**\n",
        "1.   Max Pooling w/ Average Pooling\n",
        "2.   Max Pooling w/o Average Pooling\n",
        "3.   Average Pooling\n",
        "4.   No pooling\n",
        "\n"
      ],
      "metadata": {
        "id": "LVEdjGgIuL7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, AveragePooling2D\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalize the image\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(256, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(512, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=20)\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "model.save_weights('/content/drive/MyDrive/pooling_2_weights.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XxHjFrdzHCN",
        "outputId": "273e8c9a-e1c9-41e2-857c-25cb6baa8e78"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: (32, 32, 3)\n",
            "Total number of training samples: 50000\n",
            "Total number of validation samples: 10000\n",
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 15s 9ms/step - loss: 1.6221 - accuracy: 0.4170\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.2267 - accuracy: 0.5665\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.0301 - accuracy: 0.6385\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.8950 - accuracy: 0.6891\n",
            "Epoch 5/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.7889 - accuracy: 0.7267\n",
            "Epoch 6/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.7063 - accuracy: 0.7584\n",
            "Epoch 7/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.6319 - accuracy: 0.7832\n",
            "Epoch 8/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.5664 - accuracy: 0.8066\n",
            "Epoch 9/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.5034 - accuracy: 0.8281\n",
            "Epoch 10/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.4424 - accuracy: 0.8509\n",
            "Epoch 11/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.3887 - accuracy: 0.8677\n",
            "Epoch 12/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.3351 - accuracy: 0.8879\n",
            "Epoch 13/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.2870 - accuracy: 0.9040\n",
            "Epoch 14/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.2408 - accuracy: 0.9204\n",
            "Epoch 15/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.1985 - accuracy: 0.9348\n",
            "Epoch 16/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.1640 - accuracy: 0.9458\n",
            "Epoch 17/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.1314 - accuracy: 0.9584\n",
            "Epoch 18/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.1068 - accuracy: 0.9668\n",
            "Epoch 19/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0864 - accuracy: 0.9728\n",
            "Epoch 20/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.0709 - accuracy: 0.9780\n",
            "Validation loss: 1.1184946298599243\n",
            "Validation accuracy: 0.761900007724762\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, AveragePooling2D\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalize the image\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(256, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(512, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=20)\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "model.save_weights('/content/drive/MyDrive/pooling_3_weights.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSf8rBCpzUzE",
        "outputId": "4b021012-a4fd-403f-a954-7f3fb0262928"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: (32, 32, 3)\n",
            "Total number of training samples: 50000\n",
            "Total number of validation samples: 10000\n",
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.7749 - accuracy: 0.3485\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.4990 - accuracy: 0.4531\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.3724 - accuracy: 0.5039\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.2691 - accuracy: 0.5460\n",
            "Epoch 5/20\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.1804 - accuracy: 0.5808\n",
            "Epoch 6/20\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 1.1097 - accuracy: 0.6070\n",
            "Epoch 7/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 1.0500 - accuracy: 0.6291\n",
            "Epoch 8/20\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 0.9973 - accuracy: 0.6482\n",
            "Epoch 9/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9496 - accuracy: 0.6688\n",
            "Epoch 10/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.9083 - accuracy: 0.6827\n",
            "Epoch 11/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.8691 - accuracy: 0.6971\n",
            "Epoch 12/20\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 0.8355 - accuracy: 0.7093\n",
            "Epoch 13/20\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 0.8011 - accuracy: 0.7199\n",
            "Epoch 14/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.7707 - accuracy: 0.7317\n",
            "Epoch 15/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.7422 - accuracy: 0.7399\n",
            "Epoch 16/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.7168 - accuracy: 0.7505\n",
            "Epoch 17/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.6914 - accuracy: 0.7594\n",
            "Epoch 18/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.6684 - accuracy: 0.7680\n",
            "Epoch 19/20\n",
            "1563/1563 [==============================] - 13s 9ms/step - loss: 0.6468 - accuracy: 0.7749\n",
            "Epoch 20/20\n",
            "1563/1563 [==============================] - 14s 9ms/step - loss: 0.6262 - accuracy: 0.7825\n",
            "Validation loss: 0.8541518449783325\n",
            "Validation accuracy: 0.7099000215530396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, AveragePooling2D\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalize the image\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, (3,3), padding='valid', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(256, (3,3), padding='valid', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(512, (3,3), padding='valid', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# initiate RMSprop optimizer\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=20)\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GlVPP9dd38LN",
        "outputId": "d298b28e-e3fa-47b1-c32c-e7c5665d6324"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 16s 0us/step\n",
            "Image shape: (32, 32, 3)\n",
            "Total number of training samples: 50000\n",
            "Total number of validation samples: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 31s 13ms/step - loss: 1.3665 - accuracy: 0.5124\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.9369 - accuracy: 0.6756\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.7388 - accuracy: 0.7466\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.5804 - accuracy: 0.8029\n",
            "Epoch 5/20\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.4317 - accuracy: 0.8559\n",
            "Epoch 6/20\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.2904 - accuracy: 0.9029\n",
            "Epoch 7/20\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.1829 - accuracy: 0.9397\n",
            "Epoch 8/20\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.1076 - accuracy: 0.9649\n",
            "Epoch 9/20\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.0662 - accuracy: 0.9782\n",
            "Epoch 10/20\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.0464 - accuracy: 0.9855\n",
            "Epoch 11/20\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.0360 - accuracy: 0.9878\n",
            "Epoch 12/20\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.0300 - accuracy: 0.9903\n",
            "Epoch 13/20\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.0267 - accuracy: 0.9916\n",
            "Epoch 14/20\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.0238 - accuracy: 0.9923\n",
            "Epoch 15/20\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.0236 - accuracy: 0.9926\n",
            "Epoch 16/20\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.0212 - accuracy: 0.9932\n",
            "Epoch 17/20\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.0221 - accuracy: 0.9932\n",
            "Epoch 18/20\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.0215 - accuracy: 0.9936\n",
            "Epoch 19/20\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.0249 - accuracy: 0.9928\n",
            "Epoch 20/20\n",
            "1563/1563 [==============================] - 21s 13ms/step - loss: 0.0227 - accuracy: 0.9933\n",
            "Validation loss: 2.5379676818847656\n",
            "Validation accuracy: 0.7121999859809875\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1a4e727957df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validation accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/pooling_4_weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, **kwds)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_nslots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_nbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_w0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0m\u001b[1;32m    425\u001b[0m                                fapl, fcpl=make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[1;32m    426\u001b[0m                                fs_persist=fs_persist, fs_threshold=fs_threshold),\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = '/content/drive/MyDrive/pooling_4_weights.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "model.save_weights('/content/drive/MyDrive/pooling_4_weights.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3q77Z1m-zDr",
        "outputId": "c277d963-a7da-421b-e87d-3e828047175f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment 5: Activation functions**\n",
        "1.   relu\n",
        "2.   sigmoid\n",
        "3.   elu\n",
        "4.   selu\n",
        "\n"
      ],
      "metadata": {
        "id": "Q2erjk5R_XRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, AveragePooling2D\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(256, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(512, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='sigmoid'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=20)\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "model.save_weights('/content/drive/MyDrive/activation_2_weights.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crMphHU7_ZvD",
        "outputId": "143cf1b5-6bc7-4027-ded4-c68670693337"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Image shape: (32, 32, 3)\n",
            "Total number of training samples: 50000\n",
            "Total number of validation samples: 10000\n",
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.2818 - accuracy: 0.1259\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0934 - accuracy: 0.2348\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.9944 - accuracy: 0.2804\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.8902 - accuracy: 0.3176\n",
            "Epoch 5/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.8066 - accuracy: 0.3417\n",
            "Epoch 6/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.7306 - accuracy: 0.3713\n",
            "Epoch 7/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6736 - accuracy: 0.3921\n",
            "Epoch 8/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6332 - accuracy: 0.4083\n",
            "Epoch 9/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5989 - accuracy: 0.4244\n",
            "Epoch 10/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5703 - accuracy: 0.4345\n",
            "Epoch 11/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5420 - accuracy: 0.4451\n",
            "Epoch 12/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5170 - accuracy: 0.4546\n",
            "Epoch 13/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4917 - accuracy: 0.4642\n",
            "Epoch 14/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4682 - accuracy: 0.4728\n",
            "Epoch 15/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4468 - accuracy: 0.4826\n",
            "Epoch 16/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4269 - accuracy: 0.4874\n",
            "Epoch 17/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4076 - accuracy: 0.4967\n",
            "Epoch 18/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3900 - accuracy: 0.5005\n",
            "Epoch 19/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3719 - accuracy: 0.5081\n",
            "Epoch 20/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3569 - accuracy: 0.5160\n",
            "Validation loss: 1.387694001197815\n",
            "Validation accuracy: 0.499099999666214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(256, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(512, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('elu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='elu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=20)\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "model.save_weights('/content/drive/MyDrive/activation_3_weights.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "oFV82eUrBYKL",
        "outputId": "d42e3c6e-8dbe-4f53-d59c-e631a4ad92a3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: (32, 32, 3)\n",
            "Total number of training samples: 50000\n",
            "Total number of validation samples: 10000\n",
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5370 - accuracy: 0.4491\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1648 - accuracy: 0.5924\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0190 - accuracy: 0.6441\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9212 - accuracy: 0.6816\n",
            "Epoch 5/20\n",
            "1256/1563 [=======================>......] - ETA: 1s - loss: 0.8461 - accuracy: 0.7080"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-f9e13f4405b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1412\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m     \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1157\u001b[0m     \"\"\"\n\u001b[1;32m   1158\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image shape: (32, 32, 3)\n",
        "Total number of training samples: 50000\n",
        "Total number of validation samples: 10000\n",
        "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
        "  super(RMSprop, self).__init__(name, **kwargs)\n",
        "Epoch 1/20\n",
        "1563/1563 [==============================] - 16s 4ms/step - loss: 1.5434 - accuracy: 0.4478\n",
        "Epoch 2/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1619 - accuracy: 0.5934\n",
        "Epoch 3/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0090 - accuracy: 0.6508\n",
        "Epoch 4/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9121 - accuracy: 0.6832\n",
        "Epoch 5/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8337 - accuracy: 0.7123\n",
        "Epoch 6/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7629 - accuracy: 0.7366\n",
        "Epoch 7/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7036 - accuracy: 0.7597\n",
        "Epoch 8/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6468 - accuracy: 0.7777\n",
        "Epoch 9/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5929 - accuracy: 0.7967\n",
        "Epoch 10/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5425 - accuracy: 0.8137\n",
        "Epoch 11/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4966 - accuracy: 0.8312\n",
        "Epoch 12/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4512 - accuracy: 0.8452\n",
        "Epoch 13/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4078 - accuracy: 0.8630\n",
        "Epoch 14/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3652 - accuracy: 0.8777\n",
        "Epoch 15/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3241 - accuracy: 0.8906\n",
        "Epoch 16/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2843 - accuracy: 0.9050\n",
        "Epoch 17/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2467 - accuracy: 0.9170\n",
        "Epoch 18/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.2143 - accuracy: 0.9297\n",
        "Epoch 19/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1810 - accuracy: 0.9408\n",
        "Epoch 20/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.1533 - accuracy: 0.9510\n",
        "Validation loss: 0.9123041033744812\n",
        "Validation accuracy: 0.762499988079071"
      ],
      "metadata": {
        "id": "NFem786eNbWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(128, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('selu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(256, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('selu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(512, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('selu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='selu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=20)\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "model.save_weights('/content/drive/MyDrive/activation_4_weights.h5')"
      ],
      "metadata": {
        "id": "aldoMuvVBhwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image shape: (32, 32, 3)\n",
        "Total number of training samples: 50000\n",
        "Total number of validation samples: 10000\n",
        "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
        "  super(RMSprop, self).__init__(name, **kwargs)\n",
        "Epoch 1/20\n",
        "1563/1563 [==============================] - 17s 4ms/step - loss: 1.4785 - accuracy: 0.4745\n",
        "Epoch 2/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 1.0993 - accuracy: 0.6148\n",
        "Epoch 3/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.9576 - accuracy: 0.6678\n",
        "Epoch 4/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8593 - accuracy: 0.7028\n",
        "Epoch 5/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7806 - accuracy: 0.7307\n",
        "Epoch 6/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7074 - accuracy: 0.7571\n",
        "Epoch 7/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6378 - accuracy: 0.7809\n",
        "Epoch 8/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.5736 - accuracy: 0.8050\n",
        "Epoch 9/20\n",
        "1563/1563 [==============================] - 7s 4ms/step - loss: 0.5108 - accuracy: 0.8271\n",
        "Epoch 10/20\n",
        "1563/1563 [==============================] - 7s 5ms/step - loss: 0.4481 - accuracy: 0.8491\n",
        "Epoch 11/20\n",
        "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3869 - accuracy: 0.8695\n",
        "Epoch 12/20\n",
        "1563/1563 [==============================] - 7s 5ms/step - loss: 0.3313 - accuracy: 0.8901\n",
        "Epoch 13/20\n",
        "1563/1563 [==============================] - 7s 5ms/step - loss: 0.2804 - accuracy: 0.9071\n",
        "Epoch 14/20\n",
        "1563/1563 [==============================] - 7s 5ms/step - loss: 0.2301 - accuracy: 0.9259\n",
        "Epoch 15/20\n",
        "1563/1563 [==============================] - 7s 4ms/step - loss: 0.1906 - accuracy: 0.9390\n",
        "Epoch 16/20\n",
        "1563/1563 [==============================] - 7s 5ms/step - loss: 0.1550 - accuracy: 0.9510\n",
        "Epoch 17/20\n",
        "1563/1563 [==============================] - 7s 4ms/step - loss: 0.1266 - accuracy: 0.9600\n",
        "Epoch 18/20\n",
        "1563/1563 [==============================] - 7s 5ms/step - loss: 0.1010 - accuracy: 0.9683\n",
        "Epoch 19/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0865 - accuracy: 0.9724\n",
        "Epoch 20/20\n",
        "1563/1563 [==============================] - 6s 4ms/step - loss: 0.0730 - accuracy: 0.9774\n",
        "Validation loss: 1.29814612865448\n",
        "Validation accuracy: 0.7268000245094299"
      ],
      "metadata": {
        "id": "E6MYhXLANXEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine Tuning**"
      ],
      "metadata": {
        "id": "o98B4zrIDG5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import cifar10\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, AveragePooling2D\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('Image shape: {0}'.format(x_train.shape[1:]))\n",
        "print('Total number of training samples: {0}'.format(x_train.shape[0]))\n",
        "print('Total number of validation samples: {0}'.format(x_test.shape[0]))\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "y_train_class = np_utils.to_categorical(y_train, 10)\n",
        "y_test_class = np_utils.to_categorical(y_test, 10)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (6,6), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(64, (6,6), padding='valid', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(AveragePooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.fit(x_train, y_train_class, batch_size=32, epochs=20)\n",
        "\n",
        "score = model.evaluate(x_test, y_test_class, verbose=0)\n",
        "print('Validation loss:', score[0])\n",
        "print('Validation accuracy:', score[1])\n",
        "\n",
        "model.save_weights('/content/drive/MyDrive/activation_2_weights.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23sqapqkDC7q",
        "outputId": "1dbb2b49-58bb-405c-dd0f-32593d5643c2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Image shape: (32, 32, 3)\n",
            "Total number of training samples: 50000\n",
            "Total number of validation samples: 10000\n",
            "Epoch 1/20\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.9242 - accuracy: 0.3009\n",
            "Epoch 2/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.6657 - accuracy: 0.4001\n",
            "Epoch 3/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5708 - accuracy: 0.4322\n",
            "Epoch 4/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5097 - accuracy: 0.4573\n",
            "Epoch 5/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4591 - accuracy: 0.4765\n",
            "Epoch 6/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4170 - accuracy: 0.4941\n",
            "Epoch 7/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3782 - accuracy: 0.5086\n",
            "Epoch 8/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3436 - accuracy: 0.5208\n",
            "Epoch 9/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3127 - accuracy: 0.5338\n",
            "Epoch 10/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2834 - accuracy: 0.5462\n",
            "Epoch 11/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2577 - accuracy: 0.5562\n",
            "Epoch 12/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2330 - accuracy: 0.5660\n",
            "Epoch 13/20\n",
            "1563/1563 [==============================] - 5s 4ms/step - loss: 1.2104 - accuracy: 0.5735\n",
            "Epoch 14/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1888 - accuracy: 0.5820\n",
            "Epoch 15/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1696 - accuracy: 0.5906\n",
            "Epoch 16/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1500 - accuracy: 0.5975\n",
            "Epoch 17/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1339 - accuracy: 0.6016\n",
            "Epoch 18/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1181 - accuracy: 0.6090\n",
            "Epoch 19/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1016 - accuracy: 0.6156\n",
            "Epoch 20/20\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.0888 - accuracy: 0.6190\n",
            "Validation loss: 1.1246403455734253\n",
            "Validation accuracy: 0.599399983882904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f_pYwv7Jgoa",
        "outputId": "1cde07e7-f819-4e47-a2f1-d7bfbb882040"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_81 (Conv2D)          (None, 32, 32, 32)        3488      \n",
            "                                                                 \n",
            " activation_81 (Activation)  (None, 32, 32, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d_79 (MaxPoolin  (None, 16, 16, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_82 (Conv2D)          (None, 11, 11, 64)        73792     \n",
            "                                                                 \n",
            " activation_82 (Activation)  (None, 11, 11, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_80 (MaxPoolin  (None, 5, 5, 64)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " average_pooling2d_26 (Avera  (None, 2, 2, 64)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten_30 (Flatten)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 25)                6425      \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 10)                260       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 83,965\n",
            "Trainable params: 83,965\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w8eUV9RDPkL"
      },
      "source": [
        "\n",
        "---\n",
        "---\n",
        "\n",
        "## Task 2: Regression\n",
        "\n",
        "Now we face a regression task instead of a classification problem. Loss function, activations, and dataset will change in the following task. Thus, instead of having one vector with the probabilities of each class, in this regression problem, the output is a single scalar. \n",
        "\n",
        "For this second task, we chose the task of estimating house prices based on input images. To get the data run the following script, which clones Ahmed and Moustafas [repository](https://github.com/emanhamed/Houses-dataset) into colmap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibukle2ODMEp"
      },
      "source": [
        "!git clone https://github.com/emanhamed/Houses-dataset\n",
        "%cd /content/Houses-dataset/Houses\\ Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ye6xldkVEV-t"
      },
      "source": [
        "This dataset contains four images of the house (kitchen, frontal, bedroom and bathroom), and attributes (number of bedrooms, number of bathrooms, zip code...). For our exercise, we only use the images of the house. We start with front door images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71urud9ZHq9d"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.random import seed\n",
        "\n",
        "house_section = 'frontal' # select between: kitchen, frontal, bedroom or bathroom\n",
        "print('We have selected {} images. You can switch to the kitchen, frontal, bedroom or bathroom images by changing house_section variable.'.format(house_section))\n",
        "images = []\n",
        "for i_im in range(1, 536):\n",
        "  image = cv2.imread(str(i_im)+'_'+house_section+'.jpg')\n",
        "  image = cv2.resize(image, (64, 64))\n",
        "  images.append(image)\n",
        "\n",
        "labels = []\n",
        "f = open('HousesInfo.txt', \"r\")\n",
        "for x in f:\n",
        "  label = (x).split(' ')[-1].split('\\n')[0]\n",
        "  labels.append(label)\n",
        "\n",
        "# Let's visualize some examples\n",
        "N=3\n",
        "start_val = 0 # pick an element for the code to plot the following N**2 values\n",
        "fig, axes = plt.subplots(N,N)\n",
        "for row in range(N):\n",
        "  for col in range(N):\n",
        "    idx = start_val+row+N*col    \n",
        "    tmp = cv2.cvtColor(images[idx],cv2.COLOR_BGR2RGB)\n",
        "    axes[row,col].imshow(tmp, cmap='gray')\n",
        "    fig.subplots_adjust(hspace=0.5)\n",
        "    target = int(labels[idx])\n",
        "    axes[row,col].set_title(str(target) + '$')\n",
        "    axes[row,col].set_xticks([])\n",
        "    axes[row,col].set_yticks([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scNfczb73nGS"
      },
      "source": [
        "Prepare the dataset for training the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FORNJ1QU2L-t"
      },
      "source": [
        "seed(1234)\n",
        "images = np.asarray(images).astype('float32')\n",
        "labels = np.asarray(labels).astype('float32')\n",
        "\n",
        "# Normalize the image\n",
        "max_price = labels.max()\n",
        "images /= 255.\n",
        "labels /= max_price\n",
        "\n",
        "random_idx = np.random.permutation(len(images))\n",
        "images = images[random_idx]\n",
        "labels = labels[random_idx]\n",
        "\n",
        "split_size_val = int(0.8*len(images))\n",
        "X_train, X_val = images[:split_size_val], images[split_size_val+1:]\n",
        "Y_train, Y_val = labels[:split_size_val], labels[split_size_val+1:]\n",
        "\n",
        "# Print shape of training and val images \n",
        "print('X_train image shape: {0}'.format(X_train.shape))\n",
        "print('X_val image shape: {0}'.format(X_val.shape))\n",
        "\n",
        "# Print shape of training and val labels \n",
        "print('Y_train labels shape: {0}'.format(Y_train.shape))\n",
        "print('Y_val labels shape: {0}'.format(Y_val.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGC6dXqdM2eU"
      },
      "source": [
        "### Problem Definition\n",
        "\n",
        "Similar to the previous task, you are asked to design a CNN architecture able to perform the estimation of house prices based on the `frontal` house image. Design a new model by changing parameters such as the number of convolutional layers, activation functions, strides, or pooling operators, among others."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE8g42x3M2Ko"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten\n",
        "tf.random.set_seed(1234)\n",
        "\n",
        "chanDim = -1\n",
        "model = Sequential()\n",
        "\n",
        "# . . . \n",
        "\n",
        "\n",
        "# Define here your architecture\n",
        "\n",
        "\n",
        "# . . . \n",
        "\n",
        "model.summary()\n",
        "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
        "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt, metrics=['mean_absolute_percentage_error'])\n",
        "\n",
        "# train the model\n",
        "print(\"[INFO] training model...\")\n",
        "model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=100, batch_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyXrXBFVQ-tL"
      },
      "source": [
        "The metric used in this problem to evaluate the performance is the same we used for training the model, the mean absolute percentage error. Mean absolute percentage error is defined as $\\frac{100}{n} \\sum_n \\frac{|\\hat{y} - y|}{|y|}$ where $y$ is the ground-truth, $\\hat{y}$ is the estimation of the model and `n` the number of elements in the set we are evaluating."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2u2qQh90C3K"
      },
      "source": [
        "prices_predicted = model.predict(X_val).flatten()\n",
        "diff = ((prices_predicted - Y_val) / Y_val) * 100\n",
        "error_mean = np.mean(np.abs(diff))\n",
        "\n",
        "print(\"Predicting house prices - Estimation Error: {:.2f}%\".format(error_mean))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2uJs6-xG6lD"
      },
      "source": [
        "**Report**:\n",
        "\n",
        "\n",
        "*   Propose a CNN architecture that has an estimation error in the validation set below 75%. \n",
        "*   Present a figure showing the training and validation loss vs the number of training epochs for different architectural design choices. Discuss the gap between the training and validation loss depending on the proposed architecture.\n",
        "*   Report a table with results when using any of the other images from the house (kitchen, bedroom, and bathroom)."
      ]
    }
  ]
}